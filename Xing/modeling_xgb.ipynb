{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "from scipy import stats\n",
    "import statistics\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_squared_error, make_scorer\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy.stats import skew, probplot, norm\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from pprint import pprint\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNetCV, LassoCV, Lasso, ElasticNet \n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score, StratifiedKFold \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from xgboost import XGBRegressor\n",
    "# from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whole_train_cleaned dataset has (89390, 94)\n",
      "whole_test_cleaned dataset has (9930, 94)\n",
      "unique_train_df dataset has (64484, 94)\n"
     ]
    }
   ],
   "source": [
    "training_df = pd.read_csv('./whole_train_cleaned.csv')\n",
    "test_df = pd.read_csv('./whole_test_cleaned.csv')\n",
    "unique_train_df = pd.read_csv('./unique_train_cleaned.csv')\n",
    "print(f'whole_train_cleaned dataset has {training_df.shape}')\n",
    "print(f'whole_test_cleaned dataset has {test_df.shape}')\n",
    "print(f'unique_train_df dataset has {unique_train_df.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dropping the columns encounter_id and patient_nbr\n",
    "training_df = training_df.drop(['encounter_id','patient_nbr'], axis = 1)\n",
    "test_df = test_df.drop(['encounter_id','patient_nbr'], axis = 1)\n",
    "unique_train_df = unique_train_df.drop(['encounter_id','patient_nbr'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89390, 92)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>time_in_hospital</th>\n",
       "      <th>num_lab_procedures</th>\n",
       "      <th>num_procedures</th>\n",
       "      <th>num_medications</th>\n",
       "      <th>number_outpatient</th>\n",
       "      <th>number_emergency</th>\n",
       "      <th>number_inpatient</th>\n",
       "      <th>number_diagnoses</th>\n",
       "      <th>readmitted</th>\n",
       "      <th>...</th>\n",
       "      <th>insulin_Up</th>\n",
       "      <th>glyburide-metformin_No</th>\n",
       "      <th>glyburide-metformin_Steady</th>\n",
       "      <th>glyburide-metformin_Up</th>\n",
       "      <th>glipizide-metformin_Steady</th>\n",
       "      <th>glimepiride-pioglitazone_Steady</th>\n",
       "      <th>metformin-rosiglitazone_Steady</th>\n",
       "      <th>metformin-pioglitazone_Steady</th>\n",
       "      <th>change_No</th>\n",
       "      <th>diabetesMed_Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45</td>\n",
       "      <td>13</td>\n",
       "      <td>88</td>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>85</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  time_in_hospital  num_lab_procedures  num_procedures  num_medications  \\\n",
       "0   65                 2                  51               3               11   \n",
       "1   45                 3                  86               1               15   \n",
       "2   45                13                  88               5               34   \n",
       "3   85                 4                  18               2               17   \n",
       "4   65                 3                  22               0               11   \n",
       "\n",
       "   number_outpatient  number_emergency  number_inpatient  number_diagnoses  \\\n",
       "0                  0                 0                 0                 4   \n",
       "1                  1                 0                 1                 9   \n",
       "2                  0                 0                 0                 9   \n",
       "3                  3                 2                 0                 9   \n",
       "4                  1                 0                 2                 6   \n",
       "\n",
       "   readmitted       ...         insulin_Up  glyburide-metformin_No  \\\n",
       "0           0       ...                  0                       1   \n",
       "1           0       ...                  0                       1   \n",
       "2           0       ...                  0                       1   \n",
       "3           0       ...                  0                       1   \n",
       "4           0       ...                  0                       1   \n",
       "\n",
       "   glyburide-metformin_Steady  glyburide-metformin_Up  \\\n",
       "0                           0                       0   \n",
       "1                           0                       0   \n",
       "2                           0                       0   \n",
       "3                           0                       0   \n",
       "4                           0                       0   \n",
       "\n",
       "   glipizide-metformin_Steady  glimepiride-pioglitazone_Steady  \\\n",
       "0                           0                                0   \n",
       "1                           0                                0   \n",
       "2                           0                                0   \n",
       "3                           0                                0   \n",
       "4                           0                                0   \n",
       "\n",
       "   metformin-rosiglitazone_Steady  metformin-pioglitazone_Steady  change_No  \\\n",
       "0                               0                              0          1   \n",
       "1                               0                              0          1   \n",
       "2                               0                              0          0   \n",
       "3                               0                              0          0   \n",
       "4                               0                              0          0   \n",
       "\n",
       "   diabetesMed_Yes  \n",
       "0                0  \n",
       "1                0  \n",
       "2                1  \n",
       "3                1  \n",
       "4                1  \n",
       "\n",
       "[5 rows x 92 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(training_df.shape)\n",
    "training_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the whole data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate the input feature and outpout feature\n",
    "X = training_df.loc[:, training_df.columns !='readmitted']\n",
    "y = training_df[['readmitted']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scalar fit the data to training inputs (X)\n",
    "scaler  = StandardScaler()\n",
    "scaler.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save it using a package called pickle\n",
    "import pickle\n",
    "scalerfile = 'scaler.sav'\n",
    "pickle.dump(scaler, open(scalerfile, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it back\n",
    "scaler = pickle.load(open(scalerfile, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the training inputs (X)\n",
    "X_tf = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying SMOTE method to balance the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({'readmitted': 1})\n",
      "New dataset shape Counter({0: 79213, 1: 79213})\n"
     ]
    }
   ],
   "source": [
    "# Data balancing applied using SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from collections import Counter\n",
    "print('Original dataset shape {}'.format(Counter(y)))\n",
    "sm = SMOTE(random_state=20)\n",
    "X_new, y_new = sm.fit_sample(X_tf, y)\n",
    "print('New dataset shape {}'.format(Counter(y_new)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train data into 80% train and 20% validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train data set in train and test sets for model training\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_new, y_new, test_size=0.20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "def calc_specificity(y_actual, y_pred, thresh):\n",
    "    # calculates specificity\n",
    "    return sum((y_pred < thresh) & (y_actual == 0)) /sum(y_actual ==0)\n",
    "\n",
    "def print_report(y_actual, y_pred, thresh):\n",
    "    \n",
    "    auc = roc_auc_score(y_actual, y_pred)\n",
    "    accuracy = accuracy_score(y_actual, (y_pred > thresh))\n",
    "    recall = recall_score(y_actual, (y_pred > thresh))\n",
    "    precision = precision_score(y_actual, (y_pred > thresh))\n",
    "    specificity = calc_specificity(y_actual, y_pred, thresh)\n",
    "    print('AUC:%.3f'%auc)\n",
    "    print('accuracy:%.3f'%accuracy)\n",
    "    print('recall:%.3f'%recall)\n",
    "    print('precision:%.3f'%precision)\n",
    "    print('specificity:%.3f'%specificity)\n",
    "    print('prevalence:%.3f'%calc_prevalence(y_actual))\n",
    "    print(' ')\n",
    "    return auc, accuracy, recall, precision, specificity\n",
    "\n",
    "def calc_prevalence(y_actual):\n",
    "    return (sum(y_actual)/len(y_actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=42, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr=LogisticRegression(random_state = 42)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Training:\n",
      "AUC:0.667\n",
      "accuracy:0.615\n",
      "recall:0.566\n",
      "precision:0.627\n",
      "specificity:0.665\n",
      "prevalence:0.499\n",
      " \n",
      "Validation:\n",
      "AUC:0.669\n",
      "accuracy:0.618\n",
      "recall:0.568\n",
      "precision:0.636\n",
      "specificity:0.669\n",
      "prevalence:0.504\n",
      " \n"
     ]
    }
   ],
   "source": [
    "thresh=0.5\n",
    "y_train_preds = lr.predict_proba(X_train)[:,1]\n",
    "y_valid_preds = lr.predict_proba(X_valid)[:,1]\n",
    "\n",
    "print('Logistic Regression')\n",
    "print('Training:')\n",
    "lr_train_auc, lr_train_accuracy, lr_train_recall, \\\n",
    "    lr_train_precision, lr_train_specificity = print_report(y_train,y_train_preds, thresh)\n",
    "print('Validation:')\n",
    "lr_valid_auc, lr_valid_accuracy, lr_valid_recall, \\\n",
    "    lr_valid_precision, lr_valid_specificity = print_report(y_valid,y_valid_preds, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the unique data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate the input feature and outpout feature\n",
    "X = unique_train_df.loc[:, unique_train_df.columns !='readmitted']\n",
    "y = unique_train_df[['readmitted']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scalar fit the data to training inputs (X)\n",
    "scaler  = StandardScaler()\n",
    "scaler.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save it using a package called pickle\n",
    "import pickle\n",
    "scalerfile = 'scaler.sav'\n",
    "pickle.dump(scaler, open(scalerfile, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it back\n",
    "scaler = pickle.load(open(scalerfile, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the training inputs (X)\n",
    "X_tf = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying SMOTE method to balance the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({'readmitted': 1})\n",
      "New dataset shape Counter({0: 59528, 1: 59528})\n"
     ]
    }
   ],
   "source": [
    "# Data balancing applied using SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from collections import Counter\n",
    "print('Original dataset shape {}'.format(Counter(y)))\n",
    "sm = SMOTE(random_state=20)\n",
    "X_new, y_new = sm.fit_sample(X_tf, y)\n",
    "print('New dataset shape {}'.format(Counter(y_new)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train data into 80% train and 20% validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train data set in train and test sets for model training\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_new, y_new, test_size=0.20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "def calc_specificity(y_actual, y_pred, thresh):\n",
    "    # calculates specificity\n",
    "    return sum((y_pred < thresh) & (y_actual == 0)) /sum(y_actual ==0)\n",
    "\n",
    "def print_report(y_actual, y_pred, thresh):\n",
    "    \n",
    "    auc = roc_auc_score(y_actual, y_pred)\n",
    "    accuracy = accuracy_score(y_actual, (y_pred > thresh))\n",
    "    recall = recall_score(y_actual, (y_pred > thresh))\n",
    "    precision = precision_score(y_actual, (y_pred > thresh))\n",
    "    specificity = calc_specificity(y_actual, y_pred, thresh)\n",
    "    print('AUC:%.3f'%auc)\n",
    "    print('accuracy:%.3f'%accuracy)\n",
    "    print('recall:%.3f'%recall)\n",
    "    print('precision:%.3f'%precision)\n",
    "    print('specificity:%.3f'%specificity)\n",
    "    print('prevalence:%.3f'%calc_prevalence(y_actual))\n",
    "    print(' ')\n",
    "    return auc, accuracy, recall, precision, specificity\n",
    "\n",
    "def calc_prevalence(y_actual):\n",
    "    return (sum(y_actual)/len(y_actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=42, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr=LogisticRegression(random_state = 42)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Training:\n",
      "AUC:0.684\n",
      "accuracy:0.628\n",
      "recall:0.592\n",
      "precision:0.639\n",
      "specificity:0.664\n",
      "prevalence:0.501\n",
      " \n",
      "Validation:\n",
      "AUC:0.683\n",
      "accuracy:0.626\n",
      "recall:0.591\n",
      "precision:0.630\n",
      "specificity:0.660\n",
      "prevalence:0.495\n",
      " \n"
     ]
    }
   ],
   "source": [
    "thresh=0.5\n",
    "y_train_preds = lr.predict_proba(X_train)[:,1]\n",
    "y_valid_preds = lr.predict_proba(X_valid)[:,1]\n",
    "\n",
    "print('Logistic Regression')\n",
    "print('Training:')\n",
    "lr_train_auc, lr_train_accuracy, lr_train_recall, \\\n",
    "    lr_train_precision, lr_train_specificity = print_report(y_train,y_train_preds, thresh)\n",
    "print('Validation:')\n",
    "lr_valid_auc, lr_valid_accuracy, lr_valid_recall, \\\n",
    "    lr_valid_precision, lr_valid_specificity = print_report(y_valid,y_valid_preds, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
