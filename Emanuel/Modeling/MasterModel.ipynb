{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "from sklearn import linear_model, svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold,\\\n",
    "    StratifiedShuffleSplit, cross_val_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score,\\\n",
    "    roc_auc_score, roc_curve, classification_report\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "#import eli5\n",
    "\n",
    "import graphviz\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns',99)\n",
    "pd.set_option('display.max_rows',300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydotplus\n",
      "Requirement already satisfied: pyparsing>=2.0.1 in /Users/Emanuel/opt/anaconda3/lib/python3.7/site-packages (from pydotplus) (2.4.2)\n",
      "Installing collected packages: pydotplus\n",
      "Successfully installed pydotplus-2.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pydotplus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Processes Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#READ CSVs\n",
    "\n",
    "#15,20,25,30 whole and unique\n",
    "\n",
    "train_unique_15_orig = pd.read_csv('dataset_diabetes/experiment/new/standardized splits/15/unique_train_cleaned_15.csv')\n",
    "train_whole_15_orig = pd.read_csv('dataset_diabetes/experiment/new/standardized splits/15/whole_train_cleaned_15.csv')\n",
    "test_unique_15_orig = pd.read_csv('dataset_diabetes/experiment/new/standardized splits/15/unique_test_cleaned_15.csv')\n",
    "\n",
    "train_unique_20_orig = pd.read_csv('dataset_diabetes/experiment/new/standardized splits/20/unique_train_cleaned_20.csv')\n",
    "train_whole_20_orig = pd.read_csv('dataset_diabetes/experiment/new/standardized splits/20/whole_train_cleaned_20.csv')\n",
    "test_unique_20_orig = pd.read_csv('dataset_diabetes/experiment/new/standardized splits/20/unique_test_cleaned_20.csv')\n",
    "\n",
    "train_unique_25_orig = pd.read_csv('dataset_diabetes/experiment/new/standardized splits/25/unique_train_cleaned_25.csv')\n",
    "train_whole_25_orig = pd.read_csv('dataset_diabetes/experiment/new/standardized splits/25/whole_train_cleaned_25.csv')\n",
    "test_unique_25_orig = pd.read_csv('dataset_diabetes/experiment/new/standardized splits/25/unique_test_cleaned_25.csv')\n",
    "\n",
    "train_unique_30_orig = pd.read_csv('dataset_diabetes/experiment/new/standardized splits/30/unique_train_cleaned_30.csv')\n",
    "train_whole_30_orig = pd.read_csv('dataset_diabetes/experiment/new/standardized splits/30/whole_train_cleaned_30.csv')\n",
    "test_unique_30_orig = pd.read_csv('dataset_diabetes/experiment/new/standardized splits/30/unique_test_cleaned_30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make copies. #Re-Run this cell to reset version and split selections.\n",
    "\n",
    "train_unique_15 = train_unique_15_orig.copy()\n",
    "test_unique_15 = test_unique_15_orig.copy()\n",
    "train_whole_15 = train_whole_15_orig.copy()\n",
    "\n",
    "train_unique_20 = train_unique_20_orig.copy()\n",
    "test_unique_20 = test_unique_20_orig.copy()\n",
    "train_whole_20 = train_whole_20_orig.copy()\n",
    "\n",
    "train_unique_25 = train_unique_25_orig.copy()\n",
    "test_unique_25 = test_unique_25_orig.copy()\n",
    "train_whole_25 = train_whole_25_orig.copy()\n",
    "\n",
    "train_unique_30 = train_unique_30_orig.copy()\n",
    "test_unique_30 = test_unique_30_orig.copy()\n",
    "train_whole_30 = train_whole_30_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_report(y_actual, y_pred, y_pred_proba):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_actual, y_pred).ravel()\n",
    "    fpr = fp/(fp+tn)\n",
    "    auc = roc_auc_score(y_actual, y_pred_proba)\n",
    "    print('AUC:%.3f'%auc)\n",
    "    print(f'False positive rate is {fpr}')\n",
    "    print('tp',tp,'fp',fp,'fn',fn,'tn',tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare dfs for model input. #Re-Run this cell to reset version and split selections.\n",
    "def prep_dfs(split,train_unique,train_whole,test_unique):\n",
    "    \n",
    "    train_unique_y = train_unique['readmitted']\n",
    "    unique_encounter_patient = train_unique[['encounter_id','patient_nbr']]\n",
    "    train_unique.drop(['readmitted','encounter_id','patient_nbr'], inplace = True, axis = 1)\n",
    "    \n",
    "    train_whole_y = train_whole['readmitted']\n",
    "    whole_encounter_patient = train_whole[['encounter_id','patient_nbr']]\n",
    "    train_whole.drop(['readmitted','encounter_id','patient_nbr'], inplace = True, axis = 1)\n",
    "    \n",
    "    test_unique_y = test_unique['readmitted']\n",
    "    test_unique.drop(['readmitted','encounter_id','patient_nbr'], inplace = True, axis = 1)\n",
    "    \n",
    "    print('{split} percent X_train_unique shape is {shape}'.format(split = split, shape = train_unique.shape))\n",
    "    print('{split} percent y_train_unique shape is {shape}'.format(split = split, shape = train_unique_y.shape))\n",
    "    print('{split} percent X_train_whole shape is {shape}'.format(split = split, shape = train_whole.shape))\n",
    "    print('{split} percent y_train_whole shape is {shape}'.format(split = split, shape = train_whole_y.shape))\n",
    "    print('{split} percent X_test_unique shape is {shape}'.format(split = split, shape = test_unique.shape))\n",
    "    print('{split} percent y_test_unique shape is {shape}'.format(split = split, shape = test_unique_y.shape))\n",
    "    print(' ')\n",
    "    \n",
    "    #appy SMOTE\n",
    "#     smt = SMOTE()\n",
    "#     train_unique, train_unique_y = smt.fit_sample(train_unique, train_unique_y)\n",
    "#     train_whole, train_whole_y = smt.fit_sample(train_whole, train_whole_y)\n",
    "\n",
    "# #     nr = NearMiss()\n",
    "# #     X_nearmissW, y_nearmissW = nr.fit_sample(X_trainW, y_trainW)\n",
    "# #     X_nearmissU, y_nearmissU = nr.fit_sample(X_trainU, y_trainU)\n",
    "\n",
    "#     print('The number of observations in WHOLE_train for each class using SMOTE is now {}'.format(np.bincount(train_whole_y)))\n",
    "#     print('The number of observations in UNIQUE_train for each class using SMOTE is now {}'.format(np.bincount(train_unique_y)))\n",
    "    #print('The number of observations in the WHOLE dataset for each class using Near Miss are now {}'.format(np.bincount(y_nearmissW)))\n",
    "    #print('The number of observations in the UNIQUE dataset for each class using Near Miss are now {}'.format(np.bincount(y_nearmissU)))\n",
    "    print(' ')\n",
    "    \n",
    "    return train_unique, train_unique_y, unique_encounter_patient, train_whole, train_whole_y,\\\n",
    "        whole_encounter_patient,test_unique,test_unique_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 percent X_train_unique shape is (61674, 91)\n",
      "15 percent y_train_unique shape is (61674,)\n",
      "15 percent X_train_whole shape is (84422, 91)\n",
      "15 percent y_train_whole shape is (84422,)\n",
      "15 percent X_test_unique shape is (13835, 91)\n",
      "15 percent y_test_unique shape is (13835,)\n",
      " \n",
      " \n",
      "20 percent X_train_unique shape is (58793, 91)\n",
      "20 percent y_train_unique shape is (58793,)\n",
      "20 percent X_train_whole shape is (79456, 91)\n",
      "20 percent y_train_whole shape is (79456,)\n",
      "20 percent X_test_unique shape is (18019, 91)\n",
      "20 percent y_test_unique shape is (18019,)\n",
      " \n",
      " \n",
      "25 percent X_train_unique shape is (55815, 91)\n",
      "25 percent y_train_unique shape is (55815,)\n",
      "25 percent X_train_whole shape is (74490, 91)\n",
      "25 percent y_train_whole shape is (74490,)\n",
      "25 percent X_test_unique shape is (22049, 91)\n",
      "25 percent y_test_unique shape is (22049,)\n",
      " \n",
      " \n",
      "30 percent X_train_unique shape is (52825, 91)\n",
      "30 percent y_train_unique shape is (52825,)\n",
      "30 percent X_train_whole shape is (69524, 91)\n",
      "30 percent y_train_whole shape is (69524,)\n",
      "30 percent X_test_unique shape is (25902, 91)\n",
      "30 percent y_test_unique shape is (25902,)\n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "#create dataframes for models. #Re-Run this cell to reset version and split selections.\n",
    "X_train_unique_15, y_train_unique_15, unique_15_encounter_patient, X_train_whole_15,\\\n",
    "y_train_whole_15, whole_15_encounter_patient,X_test_unique_15,y_test_unique_15 = prep_dfs(15,train_unique_15,\\\n",
    "                                                                       train_whole_15,\\\n",
    "                                                                       test_unique_15)\n",
    "\n",
    "X_train_unique_20, y_train_unique_20, unique_20_encounter_patient, X_train_whole_20,\\\n",
    "y_train_whole_20, whole_20_encounter_patient,X_test_unique_20,y_test_unique_20 = prep_dfs(20,train_unique_20,\\\n",
    "                                                                       train_whole_20,\\\n",
    "                                                                       test_unique_20)\n",
    "\n",
    "X_train_unique_25, y_train_unique_25, unique_25_encounter_patient, X_train_whole_25,\\\n",
    "y_train_whole_25, whole_25_encounter_patient,X_test_unique_25,y_test_unique_25 = prep_dfs(25,train_unique_25,\\\n",
    "                                                                        train_whole_25,\\\n",
    "                                                                        test_unique_25)\n",
    "\n",
    "X_train_unique_30, y_train_unique_30, unique_30_encounter_patient, X_train_whole_30,\\\n",
    "y_train_whole_30, whole_30_encounter_patient,X_test_unique_30,y_test_unique_30 = prep_dfs(30,train_unique_30,\\\n",
    "                                                                       train_whole_30,\\\n",
    "                                                                       test_unique_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary to store all the different dataframes with different splits.\n",
    "\n",
    "#Re-Run this cell to reset version and split selections.\n",
    "\n",
    "versions = {'X_train':{'15w':X_train_whole_15,'15u':X_train_unique_15,\\\n",
    "                       '20w':X_train_whole_20,'20u':X_train_unique_20,\\\n",
    "                       '25w':X_train_whole_25,'25u':X_train_unique_25,\\\n",
    "                       '30w':X_train_whole_30,'30u':X_train_unique_30},\\\n",
    "            'y_train':{'15w':y_train_whole_15,'15u':y_train_unique_15,\\\n",
    "                       '20w':y_train_whole_20,'20u':y_train_unique_20,\\\n",
    "                       '25w':y_train_whole_25,'25u':y_train_unique_25,\\\n",
    "                       '30w':y_train_whole_30,'30u':y_train_unique_30},\n",
    "            'X_test':{ '15u':X_test_unique_15,\\\n",
    "                       '20u':X_test_unique_20,\\\n",
    "                       '25u':X_test_unique_25,\\\n",
    "                       '30u':X_test_unique_30},\\\n",
    "            'y_test':{ '15u':y_test_unique_15,\\\n",
    "                       '20u':y_test_unique_20,\\\n",
    "                       '25u':y_test_unique_25,\\\n",
    "                       '30u':y_test_unique_30}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare which version to run in model below:\n",
    "\n",
    "def choose_version(data_split,selection):\n",
    "\n",
    "    X_train = versions['X_train'][str(data_split)+selection] \n",
    "    y_train = versions['y_train'][str(data_split)+selection]\n",
    "    X_test = versions['X_test'][str(data_split)+'u']\n",
    "    y_test = versions['y_test'][str(data_split)+'u']\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_smote(X_train,y_train):\n",
    "    \n",
    "    smt = SMOTE()\n",
    "    X_train_SMOTE, y_train_SMOTE = smt.fit_sample(X_train, y_train)\n",
    "    \n",
    "    print('The number of observations for each class using SMOTE is now {}'.format(np.bincount(y_train_SMOTE)))\n",
    "    return X_train_SMOTE, y_train_SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ==========SELECT SPLIT and VERSION, and SMOTE=========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#++++++++++++++++RUN THIS CELL TO SET WHICH SPLIT VERSION:++++++++++++++++++++\n",
    "\n",
    "X_train, y_train, X_test, y_test = choose_version(15,'w')\n",
    "\n",
    "# X_train, y_train, X_test, y_test = choose_version(25,'u')\n",
    "# X_train, y_train, X_test, y_test = choose_version(30,'w')\n",
    "\n",
    "#++++++++++++++++RUN THIS CELL TO SET WHICH SPLIT VERSION:++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of observations for each class using SMOTE is now [74836 74836]\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "###ONLY RUN IF APPLYING SMOTE!!#####\n",
    "####################################\n",
    "X_train_SMOTE, y_train_SMOTE = apply_smote(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ====================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELING \n",
    "**(Logistic Regression, Decision Tree, Random Forest, XGBoost, LightGBM)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(fit_intercept=True, penalty='l2')\n",
    "logreg.fit(X_train, y_train)\n",
    "y_test_predict = logreg.predict(X_test)\n",
    "pd.crosstab(pd.Series(y_test, name = 'Actual'), pd.Series(y_test_predict, name = 'Predict'), margins = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Classification Report\n",
    "report=classification_report(y_test, y_test_predict)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_test_predict).ravel()\n",
    "print(tn, fp, fn, tp)  # 1 1 1 1\n",
    "\n",
    "TPR=tp/(tp+fn)\n",
    "FPR=fp/(fp+tn)\n",
    "TNR=tn/(fp+tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy is {0:.3f}\".format(accuracy_score(y_test, y_test_predict)))\n",
    "print(\"Precision is {0:.3f}\".format(precision_score(y_test, y_test_predict)))\n",
    "print(\"Recall is {0:.3f}\".format(recall_score(y_test, y_test_predict)))\n",
    "print(\"AUC is {0:.3f}\".format(roc_auc_score(y_test, y_test_predict)))\n",
    "print(\"TPR is {0:.3f}\".format(TPR))\n",
    "print(\"FPR is {0:.3f}\".format(FPR))\n",
    "print(\"TNR is {0:.3f}\".format(TNR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_logreg = accuracy_score(y_test, y_test_predict)\n",
    "precision_logreg = precision_score(y_test, y_test_predict)\n",
    "recall_logreg = recall_score(y_test, y_test_predict)\n",
    "auc_logreg = roc_auc_score(y_test, y_test_predict)\n",
    "TPR_logreg=TPR\n",
    "FPR_logreg=FPR\n",
    "TNR_logreg=TNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_base=pd.DataFrame(zip(X_train.columns, np.transpose(logreg.coef_)),columns=['Features','Coefficients']).sort_values(by='Coefficients',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_base.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the crossvalidation following by fit the SMOTE balanced data\n",
    "\n",
    "logreg = LogisticRegression(solver = 'liblinear',fit_intercept=True, penalty='l1')\n",
    "logreg.fit(X_train_SMOTE, y_train_SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check with the validation testset\n",
    "y_test_predict = logreg.predict(X_test)\n",
    "probability = logreg.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(pd.Series(y_test, name = 'Actual'), pd.Series(y_test_predict, name = 'Predict'), margins = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report=classification_report(y_test, y_test_predict)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_test_predict).ravel()\n",
    "print(tn, fp, fn, tp)  # 1 1 1 1\n",
    "\n",
    "TPR=tp/(tp+fn)\n",
    "FPR=fp/(fp+tn)\n",
    "TNR=tn/(fp+tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy is {0:.3f}\".format(accuracy_score(y_test, y_test_predict)))\n",
    "print(\"Precision is {0:.3f}\".format(precision_score(y_test, y_test_predict)))\n",
    "print(\"Recall is {0:.3f}\".format(recall_score(y_test, y_test_predict)))\n",
    "print(\"AUC is {0:.3f}\".format(roc_auc_score(y_test, y_test_predict)))\n",
    "print(\"TPR is {0:.3f}\".format(TPR))\n",
    "print(\"FPR is {0:.3f}\".format(FPR))\n",
    "print(\"TNR is {0:.3f}\".format(TNR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_logreg = accuracy_score(y_test, y_test_predict)\n",
    "precision_logreg = precision_score(y_test, y_test_predict)\n",
    "recall_logreg = recall_score(y_test, y_test_predict)\n",
    "auc_logreg = roc_auc_score(y_test, y_test_predict)\n",
    "TPR_logreg=TPR\n",
    "FPR_logreg=FPR\n",
    "TNR_logreg=TNR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply 'Class_weight' = balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver = 'liblinear',fit_intercept=True, penalty='l1',class_weight='balanced')\n",
    "logreg.fit(X_train_SMOTE, y_train_SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check with the validation testset\n",
    "y_test_predict = logreg.predict(X_test)\n",
    "probability = logreg.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(pd.Series(y_test, name = 'Actual'), pd.Series(y_test_predict, name = 'Predict'), margins = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report=classification_report(y_test, y_test_predict)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_test_predict).ravel()\n",
    "print(tn, fp, fn, tp)  # 1 1 1 1\n",
    "\n",
    "TPR=tp/(tp+fn)\n",
    "FPR=fp/(fp+tn)\n",
    "TNR=tn/(fp+tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy is {0:.3f}\".format(accuracy_score(y_test, y_test_predict)))\n",
    "print(\"Precision is {0:.3f}\".format(precision_score(y_test, y_test_predict)))\n",
    "print(\"Recall is {0:.3f}\".format(recall_score(y_test, y_test_predict)))\n",
    "print(\"AUC is {0:.3f}\".format(roc_auc_score(y_test, y_test_predict)))\n",
    "print(\"TPR is {0:.3f}\".format(TPR))\n",
    "print(\"FPR is {0:.3f}\".format(FPR))\n",
    "print(\"TNR is {0:.3f}\".format(TNR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_logreg = accuracy_score(y_test, y_test_predict)\n",
    "precision_logreg = precision_score(y_test, y_test_predict)\n",
    "recall_logreg = recall_score(y_test, y_test_predict)\n",
    "auc_logreg = roc_auc_score(y_test, y_test_predict)\n",
    "TPR_logreg=TPR\n",
    "FPR_logreg=FPR\n",
    "TNR_logreg=TNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_SMOTE=pd.DataFrame(zip(X_train_SMOTE.columns, np.transpose(logreg.coef_)),columns=['Features','Coefficients']).sort_values(by='Coefficients',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_SMOTE.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply RandomizedSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply randomizedsearch to find the optimum \"C\"\n",
    "\n",
    "from scipy.stats import uniform\n",
    "\n",
    "logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200,\n",
    "                              random_state=0)\n",
    "distributions = dict(C=uniform(loc=0, scale=4),\n",
    "                     penalty=['l2', 'l1'])\n",
    "clf = RandomizedSearchCV(logistic, distributions, random_state=0)\n",
    "search = clf.fit(X_train_SMOTE, y_train_SMOTE)\n",
    "search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver = 'liblinear',fit_intercept=True, penalty='l1',class_weight='balanced',C=2.195254015709299)\n",
    "logreg.fit(X_train_SMOTE, y_train_SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check with the validation testset\n",
    "y_test_predict = logreg.predict(X_test)\n",
    "probability = logreg.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(pd.Series(y_test, name = 'Actual'), pd.Series(y_test_predict, name = 'Predict'), margins = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report=classification_report(y_test, y_test_predict)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_test_predict).ravel()\n",
    "print(tn, fp, fn, tp)  # 1 1 1 1\n",
    "\n",
    "TPR=tp/(tp+fn)\n",
    "FPR=fp/(fp+tn)\n",
    "TNR=tn/(fp+tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy is {0:.3f}\".format(accuracy_score(y_test, y_test_predict)))\n",
    "print(\"Precision is {0:.3f}\".format(precision_score(y_test, y_test_predict)))\n",
    "print(\"Recall is {0:.3f}\".format(recall_score(y_test, y_test_predict)))\n",
    "print(\"AUC is {0:.3f}\".format(roc_auc_score(y_test, y_test_predict)))\n",
    "print(\"TPR is {0:.3f}\".format(TPR))\n",
    "print(\"FPR is {0:.3f}\".format(FPR))\n",
    "print(\"TNR is {0:.3f}\".format(TNR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_logreg = accuracy_score(y_test, y_test_predict)\n",
    "precision_logreg = precision_score(y_test, y_test_predict)\n",
    "recall_logreg = recall_score(y_test, y_test_predict)\n",
    "auc_logreg = roc_auc_score(y_test, y_test_predict)\n",
    "TPR_logreg=TPR\n",
    "FPR_logreg=FPR\n",
    "TNR_logreg=TNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_random_search=pd.DataFrame(zip(X_train_SMOTE.columns, np.transpose(logreg.coef_)),columns=['Features','Coefficients']).sort_values(by='Coefficients',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_random_search.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Clssifier - \"Entropy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_en = DecisionTreeClassifier(max_depth=28, criterion = \"entropy\", min_samples_split=10)\n",
    "dt_en.fit(X_train_SMOTE, y_train_SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predict = dt_en.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(pd.Series(y_test, name = 'Actual'), pd.Series(y_test_predict, name = 'Predict'), margins = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report=classification_report(y_test, y_test_predict)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_test_predict).ravel()\n",
    "print(tn, fp, fn, tp)  # 1 1 1 1\n",
    "\n",
    "TPR=tp/(tp+fn)\n",
    "FPR=fp/(fp+tn)\n",
    "TNR=tn/(fp+tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy is {0:.3f}\".format(accuracy_score(y_test, y_test_predict)))\n",
    "print(\"Precision is {0:.3f}\".format(precision_score(y_test, y_test_predict)))\n",
    "print(\"Recall is {0:.3f}\".format(recall_score(y_test, y_test_predict)))\n",
    "print(\"AUC is {0:.3f}\".format(roc_auc_score(y_test, y_test_predict)))\n",
    "print(\"TPR is {0:.3f}\".format(TPR))\n",
    "print(\"FPR is {0:.3f}\".format(FPR))\n",
    "print(\"TNR is {0:.3f}\".format(TNR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_dt_en = accuracy_score(y_test, y_test_predict)\n",
    "precision_dt_en = precision_score(y_test, y_test_predict)\n",
    "recall_dt_en = recall_score(y_test, y_test_predict)\n",
    "auc_dt_en = roc_auc_score(y_test, y_test_predict)\n",
    "TPR_dt_en=TPR\n",
    "FPR_ldt_en=FPR\n",
    "TNR_dt_en=TNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_dt_q2 = tree.export_graphviz(dt_en, out_file=\"dt_q2.dot\", feature_names=X_train.columns, max_depth=2,\n",
    "                                 class_names=[\"No\",\"Readm\"], filled=True, rounded=True, special_characters=True)\n",
    "graph_dt_q2 = pydotplus.graph_from_dot_file('dt_q2.dot')\n",
    "Image(graph_dt_q2.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of top most features based on importance\n",
    "feature_names = X_train.columns\n",
    "feature_imports = dt_en.feature_importances_\n",
    "most_imp_features = pd.DataFrame([f for f in zip(feature_names,feature_imports)], columns=[\"Feature\", \"Importance\"]).nlargest(10, \"Importance\")\n",
    "most_imp_features.sort_values(by=\"Importance\", inplace=True)\n",
    "print(most_imp_features)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(range(len(most_imp_features)), most_imp_features.Importance, align='center', alpha=0.8)\n",
    "plt.yticks(range(len(most_imp_features)), most_imp_features.Feature, fontsize=14)\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Most important features - Decision Tree (entropy function)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier : \"Gini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_gini = RandomForestClassifier(n_estimators = 10, max_depth=25, criterion = \"gini\", min_samples_split=10)\n",
    "rf_gini.fit(X_train_SMOTE, y_train_SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predict = rf_gini.predict(X_test)\n",
    "\n",
    "pd.crosstab(pd.Series(y_test, name = 'Actual'), pd.Series(y_test_predict, name = 'Predict'), margins = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report=classification_report(y_test, y_test_predict)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_test_predict).ravel()\n",
    "print(tn, fp, fn, tp)  # 1 1 1 1\n",
    "\n",
    "TPR=tp/(tp+fn)\n",
    "FPR=fp/(fp+tn)\n",
    "TNR=tn/(fp+tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy is {0:.3f}\".format(accuracy_score(y_test, y_test_predict)))\n",
    "print(\"Precision is {0:.3f}\".format(precision_score(y_test, y_test_predict)))\n",
    "print(\"Recall is {0:.3f}\".format(recall_score(y_test, y_test_predict)))\n",
    "print(\"AUC is {0:.3f}\".format(roc_auc_score(y_test, y_test_predict)))\n",
    "print(\"TPR is {0:.3f}\".format(TPR))\n",
    "print(\"FPR is {0:.3f}\".format(FPR))\n",
    "print(\"TNR is {0:.3f}\".format(TNR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_rf_g = accuracy_score(y_test, y_test_predict)\n",
    "precision_rf_g = precision_score(y_test, y_test_predict)\n",
    "recall_rf_g = recall_score(y_test, y_test_predict)\n",
    "auc_rf_g = roc_auc_score(y_test, y_test_predict)\n",
    "\n",
    "TPR_rf_g=TPR\n",
    "FPR_rf_g=FPR\n",
    "TNR_rf_g=TNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of top most features based on importance\n",
    "feature_names = X_train.columns\n",
    "feature_imports = dt_en.feature_importances_\n",
    "most_imp_features = pd.DataFrame([f for f in zip(feature_names,feature_imports)], columns=[\"Feature\", \"Importance\"]).nlargest(10, \"Importance\")\n",
    "most_imp_features.sort_values(by=\"Importance\", inplace=True)\n",
    "print(most_imp_features)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(range(len(most_imp_features)), most_imp_features.Importance, align='center', alpha=0.8)\n",
    "plt.yticks(range(len(most_imp_features)), most_imp_features.Feature, fontsize=14)\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Most important features - Random Forest (Gini)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier : \"Entropy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_en = RandomForestClassifier(n_estimators = 10, max_depth=25, criterion = \"entropy\", min_samples_split=10)\n",
    "rf_en.fit(X_train_SMOTE, y_train_SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predict = rf_en.predict(X_test)\n",
    "pd.crosstab(pd.Series(y_test, name = 'Actual'), pd.Series(y_test_predict, name = 'Predict'), margins = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report=classification_report(y_test, y_test_predict)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_test_predict).ravel()\n",
    "print(tn, fp, fn, tp)  # 1 1 1 1\n",
    "\n",
    "TPR=tp/(tp+fn)\n",
    "FPR=fp/(fp+tn)\n",
    "TNR=tn/(fp+tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy is {0:.3f}\".format(accuracy_score(y_test, y_test_predict)))\n",
    "print(\"Precision is {0:.3f}\".format(precision_score(y_test, y_test_predict)))\n",
    "print(\"Recall is {0:.3f}\".format(recall_score(y_test, y_test_predict)))\n",
    "print(\"AUC is {0:.3f}\".format(roc_auc_score(y_test, y_test_predict)))\n",
    "print(\"TPR is {0:.3f}\".format(TPR))\n",
    "print(\"FPR is {0:.3f}\".format(FPR))\n",
    "print(\"TNR is {0:.3f}\".format(TNR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_rf_en = accuracy_score(y_test, y_test_predict)\n",
    "precision_rf_en = precision_score(y_test, y_test_predict)\n",
    "recall_rf_en = recall_score(y_test, y_test_predict)\n",
    "auc_rf_en = roc_auc_score(y_test, y_test_predict)\n",
    "\n",
    "TPR_rf_en=TPR\n",
    "FPR_rf_en=FPR\n",
    "TNR_rf_en=TNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of top most features based on importance\n",
    "feature_names = X_train.columns\n",
    "feature_imports = rf_en.feature_importances_\n",
    "most_imp_features = pd.DataFrame([f for f in zip(feature_names,feature_imports)], columns=[\"Feature\", \"Importance\"]).nlargest(10, \"Importance\")\n",
    "most_imp_features.sort_values(by=\"Importance\", inplace=True)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(range(len(most_imp_features)), most_imp_features.Importance, align='center', alpha=0.8)\n",
    "plt.yticks(range(len(most_imp_features)), most_imp_features.Feature, fontsize=14)\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Most important features - Random Forest (entropy)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xgbc = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit X_train, y_train\n",
    "xgbc.fit(X_train_SMOTE,y_train_SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh=0.5\n",
    "y_train_pred_proba = xgbc.predict_proba(X_train_SMOTE)[:,1]\n",
    "y_test_pred_proba = xgbc.predict_proba(X_test)[:,1]\n",
    "\n",
    "y_train_pred = xgbc.predict(X_train_SMOTE)\n",
    "y_test_pred = xgbc.predict(X_test)\n",
    "\n",
    "print('XGBoost')\n",
    "print('Training:')\n",
    "print_report(y_train_SMOTE, y_train_pred, y_train_pred_proba)\n",
    "# print('Validation:')\n",
    "# print_report(y_valid, y_valid_pred, y_valid_pred_proba)\n",
    "print('')\n",
    "print('Test:')\n",
    "print_report(y_test, y_test_pred, y_test_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(X_train_SMOTE.columns,xgbc.feature_importances_), key= lambda t:t[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **parameter tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc1 = xgb.XGBClassifier()\n",
    "xgbc1.set_params(learning_rate=0.1,subsample=0.8,n_estimators=3000,max_depth=6,\n",
    "                colsample_bytree=1,objective='binary:logistic', n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit X_train, y_train\n",
    "xgbc1.fit(X_train_SMOTE,y_train_SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc1.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_proba = xgbc1.predict_proba(X_train_SMOTE)[:,1]\n",
    "# y_valid_pred_proba = xgbc1.predict_proba(X_valid)[:,1]\n",
    "y_test_pred_proba = xgbc1.predict_proba(X_test)[:,1]\n",
    "\n",
    "y_train_pred = xgbc1.predict(X_train_SMOTE)\n",
    "# y_valid_pred = xgbc1.predict(X_valid)\n",
    "y_test_pred = xgbc1.predict(X_test)\n",
    "\n",
    "print('Tuned XGBoost')\n",
    "print('Training:')\n",
    "print_report(y_train_SMOTE, y_train_pred, y_train_pred_proba)\n",
    "# print('Validation:')\n",
    "# print_report(y_valid, y_valid_pred, y_valid_pred_proba)\n",
    "print('Test:')\n",
    "print_report(y_test, y_test_pred, y_test_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train_SMOTE, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(X_train_SMOTE.columns,xgbc1.feature_importances_), key= lambda t:t[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(xgbc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test['predicted_probaility'] = y_test_pred_proba\n",
    "# X_test['actual_readmitted'] = y_test\n",
    "# X_test['predicted_readmitted'] = y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.sort_values(by='predicted_probaility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the quantiles as another columns\n",
    "# X_test['Quantile_rank'] = pd.qcut(X_test['predicted_probaility'],10,labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Base lightgbm (with SMOTE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del X_test['predicted_probaility']\n",
    "# del X_test['actual_readmitted']\n",
    "# del X_test['predicted_readmitted']\n",
    "# del X_test['Quantile_rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbbase = lgb.LGBMClassifier()\n",
    "lgbbase.fit(X_train_SMOTE,y_train_SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "#print('='*25,'Results','='*25)\n",
    "print('')\n",
    "#print(\"Score best parameters: \", min(loss)*-1)\n",
    "#print(\"Best parameters: \", best_param)\n",
    "#print(\"Test Score: \", clf_best.score(X_test, y_test))\n",
    "#print(\"Time elapsed: \", time.time() - start)\n",
    "#print(\"Parameter combinations evaluated: \", num_eval)\n",
    "auc = roc_auc_score(y_test, lgbbase.predict_proba(X_test)[:,1])\n",
    "auct = roc_auc_score(y_train_SMOTE, lgbbase.predict_proba(X_train_SMOTE)[:,1])\n",
    "print('AUC train is: ',auct)\n",
    "print('AUC test is: ',auc)\n",
    "print_report(y_test,lgbbase.predict(X_test),lgbbase.predict_proba(X_test)[:,1])\n",
    "cmlgbb = pd.DataFrame(confusion_matrix(y_test,lgbbase.predict(X_test)))\n",
    "cmlgbb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train_SMOTE, clf_best.predict(X_train_SMOTE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tune LightGBM SMOTE: Bayesian Optimization** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare objective function for the optimizer\n",
    "def objective_function(params):\n",
    "    clf = lgb.LGBMClassifier(**params)\n",
    "    skf = ms.StratifiedKFold(n_splits=10, shuffle=True, random_state=99)\n",
    "    score = cross_val_score(clf, X_train_SMOTE, y_train_SMOTE, scoring = 'roc_auc', cv=skf, n_jobs=1)\n",
    "    return {'loss': 1-score.mean(), 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter space and minimizer\n",
    "\n",
    "num_eval = 100\n",
    "#params to consider\n",
    "param_hyperopt= {\n",
    "                #'is_unbalanced': True,\n",
    "                'objective':'binary',\n",
    "                'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(0.09)),\n",
    "                'max_depth': scope.int(hp.quniform('max_depth', 25, 80, 1)), #5,19,1\n",
    "                'min_data_in_leaf':scope.int(hp.quniform('min_data_in_leaf', 50, 60, 5)),\n",
    "                'n_estimators': scope.int(hp.quniform('n_estimators', 100, 200, 20)), #5,55,1  180,220\n",
    "                'num_leaves': scope.int(hp.quniform('num_leaves', 100, 160, 1)), #5,50,1\n",
    "                'boosting_type': 'goss', #hp.choice('boosting_type', ['gbdt','dart','goss']), #check lightgbm for types\n",
    "                'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0),\n",
    "                'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.5), #ridge\n",
    "                #'drop_rate': hp.uniform('drop_rate', 0.0, 1.0),\n",
    "                'reg_alpha': hp.uniform('reg_alpha', 0.10, 0.15),\n",
    "                #'max_bin': scope.int(hp.quniform('max_bin', 255, 265, 1)),\n",
    "                #'scale_pos_weight': hp.uniform('scale_pos_weight', 1,1.1),\n",
    "                #'early_stopping_round': scope.int(hp.quniform('early_stopping_round', 100, 101, 1)),\n",
    "                'metric': 'auc'\n",
    "                }\n",
    "\n",
    "#loss minimizer, then store parameters\n",
    "trials = Trials()\n",
    "best_param = fmin(objective_function, \n",
    "                  param_hyperopt, \n",
    "                  algo=tpe.suggest, \n",
    "                  max_evals=num_eval, \n",
    "                  trials=trials,\n",
    "                  rstate= np.random.RandomState(1))\n",
    "loss = [x['result']['loss'] for x in trials.trials]\n",
    "best_param_values = [x for x in best_param.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit based off the optimizer above\n",
    "\n",
    "if best_param_values[0] == 0:\n",
    "    boosting_type = 'gbdt'\n",
    "\n",
    "elif best_param_values[0] == 1:\n",
    "    boosting_type = 'dart'\n",
    "\n",
    "else:\n",
    "    #best_param_values[0] == 2\n",
    "    boosting_type = 'goss'    \n",
    "    \n",
    "# else:\n",
    "#     boosting_type = 'goss'\n",
    "        \n",
    "    \n",
    "clf_smote = lgb.LGBMClassifier(\n",
    "                              #is_unbalanced = True,\n",
    "                              boosting_type=boosting_type,\n",
    "                              colsample_bytree = best_param['colsample_bytree'],\n",
    "                              #drop_rate = best_param['drop_rate'],\n",
    "                              learning_rate = best_param['learning_rate'],\n",
    "                              #max_bin = int(best_param['max_bin']),\n",
    "                              min_data_in_leaf = int(best_param['min_data_in_leaf']),\n",
    "                              max_depth = int(best_param['max_depth']),\n",
    "                              n_estimators = int(best_param['n_estimators']),\n",
    "                              num_leaves = int(best_param['num_leaves']),\n",
    "                              reg_alpha = best_param['reg_alpha'],\n",
    "                              reg_lambda = best_param['reg_lambda'],\n",
    "                              objective = 'binary',\n",
    "                              metric = 'auc',\n",
    "                              #scale_pos_weight = 1 #best_param['scale_pos_weight'], #4\n",
    "#                               bagging_frequency = 1,\n",
    "#                               pos_bagging_fraction = .99,\n",
    "#                               neg_bagging_fraction = .3\n",
    "                              \n",
    "                              #early_stopping_round = int(best_param['early_stopping_round'])\n",
    "                              )\n",
    "                                  \n",
    "clf_smote.fit(X_train_SMOTE, y_train_SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print(\"\")\n",
    "print('='*25,'Results','='*25)\n",
    "print('')\n",
    "print(\"Score best parameters: \", min(loss)*-1)\n",
    "print(\"Best parameters: \", best_param)\n",
    "print(\"Test Score: \", clf_smote.score(X_test, y_test))\n",
    "#print(\"Time elapsed: \", time.time() - start)\n",
    "print(\"Parameter combinations evaluated: \", num_eval)\n",
    "auc = roc_auc_score(y_test, clf_smote.predict_proba(X_test)[:,1])\n",
    "auct = roc_auc_score(y_train, clf_smote.predict_proba(X_train)[:,1])\n",
    "print('AUC train is: ',auct)\n",
    "print('AUC test is: ',auc)\n",
    "print_report(y_test,clf_smote.predict(X_test),clf_smote.predict_proba(X_test)[:,1])\n",
    "cmlgbm = pd.DataFrame(confusion_matrix(y_test,clf_smote.predict(X_test)))\n",
    "cmlgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tune LightGBM NO SMOTE: Bayesian Optimization** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eval = 100\n",
    "#params to consider\n",
    "param_hyperopt= {\n",
    "                'is_unbalanced': True,\n",
    "                'objective':'binary',\n",
    "                'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(0.09)),\n",
    "                'max_depth': scope.int(hp.quniform('max_depth', 25, 80, 1)), #5,19,1\n",
    "                'min_data_in_leaf':scope.int(hp.quniform('min_data_in_leaf', 50, 60, 5)),\n",
    "                'n_estimators': scope.int(hp.quniform('n_estimators', 100, 300, 20)), #5,55,1  180,220\n",
    "                'num_leaves': scope.int(hp.quniform('num_leaves', 100, 160, 1)), #5,50,1\n",
    "                'boosting_type': 'goss', #hp.choice('boosting_type', ['gbdt','dart','goss']), #check lightgbm for types\n",
    "                'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0),\n",
    "                'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.5), #ridge\n",
    "                #'drop_rate': hp.uniform('drop_rate', 0.0, 1.0),\n",
    "                'reg_alpha': hp.uniform('reg_alpha', 0.10, 0.15),\n",
    "                #'max_bin': scope.int(hp.quniform('max_bin', 255, 265, 1)),\n",
    "                #'scale_pos_weight': hp.uniform('scale_pos_weight', 1,1.1),\n",
    "                #'early_stopping_round': scope.int(hp.quniform('early_stopping_round', 100, 101, 1)),\n",
    "                'metric': 'auc'\n",
    "                }\n",
    "\n",
    "#loss minimizer, then store parameters\n",
    "trials = Trials()\n",
    "best_param = fmin(objective_function, \n",
    "                  param_hyperopt, \n",
    "                  algo=tpe.suggest, \n",
    "                  max_evals=num_eval, \n",
    "                  trials=trials,\n",
    "                  rstate= np.random.RandomState(1))\n",
    "loss = [x['result']['loss'] for x in trials.trials]\n",
    "best_param_values = [x for x in best_param.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_param_values[0] == 0:\n",
    "    boosting_type = 'gbdt'\n",
    "\n",
    "elif best_param_values[0] == 1:\n",
    "    boosting_type = 'dart'\n",
    "\n",
    "else:\n",
    "    #best_param_values[0] == 2\n",
    "    boosting_type = 'goss'    \n",
    "    \n",
    "# else:\n",
    "#     boosting_type = 'goss'\n",
    "        \n",
    "    \n",
    "clf_best = lgb.LGBMClassifier(\n",
    "                              #is_unbalanced = True,\n",
    "                              boosting_type=boosting_type,\n",
    "                              colsample_bytree = best_param['colsample_bytree'],\n",
    "                              #drop_rate = best_param['drop_rate'],\n",
    "                              learning_rate = best_param['learning_rate'],\n",
    "                              #max_bin = int(best_param['max_bin']),\n",
    "                              min_data_in_leaf = int(best_param['min_data_in_leaf']),\n",
    "                              max_depth = int(best_param['max_depth']),\n",
    "                              n_estimators = int(best_param['n_estimators']),\n",
    "                              num_leaves = int(best_param['num_leaves']),\n",
    "                              reg_alpha = best_param['reg_alpha'],\n",
    "                              reg_lambda = best_param['reg_lambda'],\n",
    "                              objective = 'binary',\n",
    "                              metric = 'auc',\n",
    "                              scale_pos_weight = 2 #best_param['scale_pos_weight'], #4\n",
    "#                               bagging_frequency = 1,\n",
    "#                               pos_bagging_fraction = .99,\n",
    "#                               neg_bagging_fraction = .3\n",
    "                              \n",
    "                              #early_stopping_round = int(best_param['early_stopping_round'])\n",
    "                              )\n",
    "                                  \n",
    "clf_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print(\"\")\n",
    "print('='*25,'Results','='*25)\n",
    "print('')\n",
    "print(\"Score best parameters: \", min(loss)*-1)\n",
    "print(\"Best parameters: \", best_param)\n",
    "print(\"Test Score: \", clf_best.score(X_test, y_test))\n",
    "#print(\"Time elapsed: \", time.time() - start)\n",
    "print(\"Parameter combinations evaluated: \", num_eval)\n",
    "auc = roc_auc_score(y_test, clf_best.predict_proba(X_test)[:,1])\n",
    "auct = roc_auc_score(y_train, clf_best.predict_proba(X_train)[:,1])\n",
    "print('AUC train is: ',auct)\n",
    "print('AUC test is: ',auc)\n",
    "print_report(y_test,clf_best.predict(X_test),clf_best.predict_proba(X_test)[:,1])\n",
    "cmlgbm = pd.DataFrame(confusion_matrix(y_test,clf_best.predict(X_test)))\n",
    "cmlgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SELECTED (70% whole train)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_chosen = lgb.LGBMClassifier(\n",
    "                              is_unbalanced = True,\n",
    "                              boosting_type='goss',\n",
    "                              colsample_bytree = 0.5937307464548665,\n",
    "                              #drop_rate = best_param['drop_rate'],\n",
    "                              learning_rate = 0.017865274565261144,\n",
    "                              #max_bin = int(best_param['max_bin']),\n",
    "                              min_data_in_leaf = 800,\n",
    "                              max_depth = 61,\n",
    "                              n_estimators = 3300,\n",
    "                              num_leaves = 320,\n",
    "                              reg_alpha = 0.1376473846645497,\n",
    "                              reg_lambda = 1.3051865999142085,\n",
    "                              objective = 'binary',\n",
    "                              metric = 'auc',\n",
    "                              #scale_pos_weight = best_param['scale_pos_weight'], #4\n",
    "#                               bagging_frequency = 1,\n",
    "#                               pos_bagging_fraction = .99,\n",
    "#                               neg_bagging_fraction = .3\n",
    "                              \n",
    "                              #early_stopping_round = int(best_param['early_stopping_round'])\n",
    "                              )\n",
    "                                  \n",
    "clf_chosen.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print(\"\")\n",
    "print('='*25,'Results','='*25)\n",
    "print('')\n",
    "print(\"Score best parameters: \", min(loss)*-1)\n",
    "#print(\"Best parameters: \", best_param)\n",
    "print(\"Test Score: \", clf_chosen.score(X_test, y_test))\n",
    "#print(\"Time elapsed: \", time.time() - start)\n",
    "print(\"Parameter combinations evaluated: \", num_eval)\n",
    "auc = roc_auc_score(y_test, clf_chosen.predict_proba(X_test)[:,1])\n",
    "auct = roc_auc_score(y_train, clf_chosen.predict_proba(X_train)[:,1])\n",
    "print('AUC train is: ',auct)\n",
    "print('AUC test is: ',auc)\n",
    "print_report(y_test,clf_chosen.predict(X_test),clf_chosen.predict_proba(X_test)[:,1])\n",
    "cmclgbm = confusion_matrix(y_test,clf_chosen.predict(X_test))\n",
    "cmclgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_weights = eli5.explain_weights(clf_chosen)\n",
    "chosen_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp = pd.DataFrame({'weights': [0.174,0.1437,0.1044,0.0756,0.0555,0.0533,0.0532,0.0463,0.025,0.0245,0.0205,0.0177,0.0148,0.0139,0.0128,0.0125,0.012,0.0111,0.011],\\\n",
    " 'features':['Inpatient Visits','Num. lab procedure','Num. .medications','Time in hospital','age','discharged to home','number of diagnoses','Num. procedures','Num. emergency','Num. medicine','Male','discharged to hospital','admitted from urgentcare','No insulin given','Num. outpatient','change_No','med. dosage change','admitted from referral','admission type urgent']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_names = feat_imp['features']\n",
    "#feature_imports = feat_imp['weights']\n",
    "#most_imp_features = pd.DataFrame([f for f in zip(feature_names,feature_imports)], columns=[\"Feature\", \"Importance\"]).nlargest(10, \"Importance\")\n",
    "feat_imp.sort_values(by=\"weights\", inplace=True, ascending = True)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(range(len(feat_imp)), feat_imp['weights'], align='center', alpha=0.8)\n",
    "plt.yticks(range(len(feat_imp)), feat_imp['features'], fontsize=14)\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Most important features by weight - LightGBM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lift(X_test,y_test,best_model):\n",
    "    \n",
    "    liftdf = X_test.copy()\n",
    "\n",
    "    liftdf['predicted_probaility'] = best_model.predict_proba(X_test)[:,1]\n",
    "    liftdf['predicted_readmitted'] = best_model.predict(X_test)\n",
    "    liftdf['actual_readmitted'] = y_test\n",
    "    #adding the quantiles as another columns\n",
    "    liftdf['Quantile_rank'] = pd.qcut(liftdf['predicted_probaility'],10,labels=np.arange(1,11))\n",
    "    \n",
    "    group = liftdf.groupby('Quantile_rank', as_index = False)['actual_readmitted'].agg(['count','sum'])\n",
    "    group.columns = ['unique patients', 'actual readmitted']\n",
    "    group.reset_index(inplace = True)\n",
    "    \n",
    "    lift_chart = liftdf.groupby('Quantile_rank')['predicted_probaility'].agg(['min','max','mean'])\n",
    "    lift_chart.columns = ['min_prob','max_prob','avg_prob']\n",
    "    lift_chart.reset_index(inplace = True)\n",
    "    \n",
    "    lift_table = pd.merge(group,lift_chart, how = 'left', on = 'Quantile_rank')\n",
    "    lift_table['pct_readmitted'] = lift_table['actual readmitted']/lift_table['unique patients']\n",
    "    \n",
    "    return lift_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lift = make_lift(X_test,y_test,clf_chosen)\n",
    "lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lift.to_csv('dataset_diabetes/liftcurrent.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot for tuned model\n",
    "\n",
    "y_score = clf_best.predict_proba(X_test)\n",
    "fpr_tuned, tpr_tuned, _ = roc_curve(y_test,  y_score[:,1])\n",
    "\n",
    "# # Plot ROC curve\n",
    "# plt.plot(fpr_base, tpr_base, label='Base ROC curve (area = %0.3f)' % auc_score_base)\n",
    "plt.plot(fpr_tuned, tpr_tuned, label='Tuned ROC curve (area = %0.3f)')# % auc_score_tuned)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate or (1 - Specifity)')\n",
    "plt.ylabel('True Positive Rate or (Sensitivity)')\n",
    "plt.title('ROC AUC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
