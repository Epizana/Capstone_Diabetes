{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn import linear_model, svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, StratifiedShuffleSplit,\\\n",
    "    cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score,\\\n",
    "    roc_auc_score\n",
    "\n",
    "import lightgbm as lgb\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns',99)\n",
    "pd.set_option('display.max_rows',300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#READ CSVs\n",
    "\n",
    "#xing\n",
    "train_unique_orig = pd.read_csv('dataset_diabetes/data2/train_unique_cleaned_df.csv')\n",
    "test_unique_orig = pd.read_csv('dataset_diabetes/data2/test_unique_cleaned_df.csv')\n",
    "\n",
    "#15,20,25,30 whole and unique\n",
    "\n",
    "train_unique_15_orig = pd.read_csv('dataset_diabetes/experiment/new/15/unique_train_cleaned_15.csv')\n",
    "train_whole_15_orig = pd.read_csv('dataset_diabetes/experiment/new/15/whole_train_cleaned_15.csv')\n",
    "test_unique_15_orig = pd.read_csv('dataset_diabetes/experiment/new/15/unique_test_cleaned_15.csv')\n",
    "\n",
    "train_unique_20_orig = pd.read_csv('dataset_diabetes/experiment/new/20/unique_train_cleaned_20.csv')\n",
    "train_whole_20_orig = pd.read_csv('dataset_diabetes/experiment/new/20/whole_train_cleaned_20.csv')\n",
    "test_unique_20_orig = pd.read_csv('dataset_diabetes/experiment/new/20/unique_test_cleaned_20.csv')\n",
    "\n",
    "train_unique_25_orig = pd.read_csv('dataset_diabetes/experiment/new/25/unique_train_cleaned_25.csv')\n",
    "train_whole_25_orig = pd.read_csv('dataset_diabetes/experiment/new/25/whole_train_cleaned_25.csv')\n",
    "test_unique_25_orig = pd.read_csv('dataset_diabetes/experiment/new/25/unique_test_cleaned_25.csv')\n",
    "\n",
    "train_unique_30_orig = pd.read_csv('dataset_diabetes/experiment/new/30/unique_train_cleaned_30.csv')\n",
    "train_whole_30_orig = pd.read_csv('dataset_diabetes/experiment/new/30/whole_train_cleaned_30.csv')\n",
    "test_unique_30_orig = pd.read_csv('dataset_diabetes/experiment/new/30/unique_test_cleaned_30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make copies\n",
    "\n",
    "train_unique_xing = train_unique_orig.copy()\n",
    "test_unique_xing = test_unique_orig.copy()\n",
    "\n",
    "\n",
    "train_unique_15 = train_unique_15_orig.copy()\n",
    "test_unique_15 = test_unique_15_orig.copy()\n",
    "train_whole_15 = train_whole_15_orig.copy()\n",
    "\n",
    "train_unique_20 = train_unique_20_orig.copy()\n",
    "test_unique_20 = test_unique_20_orig.copy()\n",
    "train_whole_20 = train_whole_20_orig.copy()\n",
    "\n",
    "train_unique_25 = train_unique_25_orig.copy()\n",
    "test_unique_25 = test_unique_25_orig.copy()\n",
    "train_whole_25 = train_whole_25_orig.copy()\n",
    "\n",
    "train_unique_30 = train_unique_30_orig.copy()\n",
    "test_unique_30 = test_unique_30_orig.copy()\n",
    "train_whole_30 = train_whole_30_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_unique_30.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_dfs(split,train_unique,train_whole,test_unique):\n",
    "    \n",
    "    train_unique_y = train_unique['readmitted']\n",
    "    unique_encounter_patient = train_unique[['encounter_id','patient_nbr']]\n",
    "    train_unique.drop(['readmitted','encounter_id','patient_nbr'], inplace = True, axis = 1)\n",
    "    \n",
    "    train_whole_y = train_whole['readmitted']\n",
    "    whole_encounter_patient = train_whole[['encounter_id','patient_nbr']]\n",
    "    train_whole.drop(['readmitted','encounter_id','patient_nbr'], inplace = True, axis = 1)\n",
    "    \n",
    "    test_unique_y = test_unique['readmitted']\n",
    "    test_unique.drop(['readmitted','encounter_id','patient_nbr'], inplace = True, axis = 1)\n",
    "    \n",
    "    print('{split} percent X_train_unique shape is {shape}'.format(split = split, shape = train_unique.shape))\n",
    "    print('{split} percent y_train_unique shape is {shape}'.format(split = split, shape = train_unique_y.shape))\n",
    "    print('{split} percent X_train_whole shape is {shape}'.format(split = split, shape = train_whole.shape))\n",
    "    print('{split} percent y_train_whole shape is {shape}'.format(split = split, shape = train_whole_y.shape))\n",
    "    print('{split} percent X_test_unique shape is {shape}'.format(split = split, shape = test_unique.shape))\n",
    "    print('{split} percent y_test_unique shape is {shape}'.format(split = split, shape = test_unique_y.shape))\n",
    "    print(' ')\n",
    "    \n",
    "    return train_unique, train_unique_y, unique_encounter_patient, train_whole, train_whole_y,\\\n",
    "        whole_encounter_patient,test_unique,test_unique_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 percent X_train_unique shape is (61674, 91)\n",
      "15 percent y_train_unique shape is (61674,)\n",
      "15 percent X_train_whole shape is (84422, 91)\n",
      "15 percent y_train_whole shape is (84422,)\n",
      "15 percent X_test_unique shape is (13835, 91)\n",
      "15 percent y_test_unique shape is (13835,)\n",
      " \n",
      "20 percent X_train_unique shape is (58793, 91)\n",
      "20 percent y_train_unique shape is (58793,)\n",
      "20 percent X_train_whole shape is (79456, 91)\n",
      "20 percent y_train_whole shape is (79456,)\n",
      "20 percent X_test_unique shape is (18019, 91)\n",
      "20 percent y_test_unique shape is (18019,)\n",
      " \n",
      "25 percent X_train_unique shape is (55815, 91)\n",
      "25 percent y_train_unique shape is (55815,)\n",
      "25 percent X_train_whole shape is (74490, 91)\n",
      "25 percent y_train_whole shape is (74490,)\n",
      "25 percent X_test_unique shape is (22049, 91)\n",
      "25 percent y_test_unique shape is (22049,)\n",
      " \n",
      "30 percent X_train_unique shape is (52825, 91)\n",
      "30 percent y_train_unique shape is (52825,)\n",
      "30 percent X_train_whole shape is (69524, 91)\n",
      "30 percent y_train_whole shape is (69524,)\n",
      "30 percent X_test_unique shape is (25902, 91)\n",
      "30 percent y_test_unique shape is (25902,)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#create dataframes for models\n",
    "X_train_unique_15, y_train_unique_15, unique_15_encounter_patient, X_train_whole_15,\\\n",
    "y_train_whole_15, whole_15_encounter_patient,X_test_unique_15,y_test_unique_15 = prep_dfs(15,train_unique_15,\\\n",
    "                                                                       train_whole_15,\\\n",
    "                                                                       test_unique_15)\n",
    "\n",
    "X_train_unique_20, y_train_unique_20, unique_20_encounter_patient, X_train_whole_20,\\\n",
    "y_train_whole_20, whole_20_encounter_patient,X_test_unique_20,y_test_unique_20 = prep_dfs(20,train_unique_20,\\\n",
    "                                                                       train_whole_20,\\\n",
    "                                                                       test_unique_20)\n",
    "\n",
    "X_train_unique_25, y_train_unique_25, unique_25_encounter_patient, X_train_whole_25,\\\n",
    "y_train_whole_25, whole_25_encounter_patient,X_test_unique_25,y_test_unique_25 = prep_dfs(25,train_unique_25,\\\n",
    "                                                                        train_whole_25,\\\n",
    "                                                                        test_unique_25)\n",
    "\n",
    "X_train_unique_30, y_train_unique_30, unique_30_encounter_patient, X_train_whole_30,\\\n",
    "y_train_whole_30, whole_30_encounter_patient,X_test_unique_30,y_test_unique_30 = prep_dfs(30,train_unique_30,\\\n",
    "                                                                       train_whole_30,\\\n",
    "                                                                       test_unique_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary to store all the different dataframes with different splits\n",
    "versions = {'X_train':{'15w':X_train_whole_15,'15u':X_train_unique_15,\\\n",
    "                       '20w':X_train_whole_20,'20u':X_train_unique_20,\\\n",
    "                       '25w':X_train_whole_25,'25u':X_train_unique_25,\\\n",
    "                       '30w':X_train_whole_30,'30u':X_train_unique_30},\\\n",
    "            'y_train':{'15w':y_train_whole_15,'15u':y_train_unique_15,\\\n",
    "                       '20w':y_train_whole_20,'20u':y_train_unique_20,\\\n",
    "                       '25w':y_train_whole_25,'25u':y_train_unique_25,\\\n",
    "                       '30w':y_train_whole_30,'30u':y_train_unique_30},\n",
    "            'X_test':{ '15u':X_test_unique_15,\\\n",
    "                       '20u':X_test_unique_20,\\\n",
    "                       '25u':X_test_unique_25,\\\n",
    "                       '30u':X_test_unique_30},\\\n",
    "            'y_test':{ '15u':y_test_unique_15,\\\n",
    "                       '20u':y_test_unique_20,\\\n",
    "                       '25u':y_test_unique_25,\\\n",
    "                       '30u':y_test_unique_30}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "#versions['X_train']['15w'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#versions['X_test']['15u'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#versions['y_test']['15u'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare which version to run in model below:\n",
    "\n",
    "def choose_version(data_split,selection):\n",
    "\n",
    "    X_train = versions['X_train'][str(data_split)+selection] \n",
    "    y_train = versions['y_train'][str(data_split)+selection]\n",
    "    X_test = versions['X_test'][str(data_split)+'u']\n",
    "    y_test = versions['y_test'][str(data_split)+'u']\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#++++++++++++++++RUN THIS CELL TO SET WHICH SPLIT VERSION:++++++++++++++++++++\n",
    "\n",
    "X_train, y_train, X_test, y_test = choose_version(30,'w')\n",
    "\n",
    "# X_train, y_train, X_test, y_test = choose_version(25,'u')\n",
    "# X_train, y_train, X_test, y_test = choose_version(30,'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(params):\n",
    "    clf = lgb.LGBMClassifier(**params)\n",
    "    skf = ms.StratifiedKFold(n_splits=10, shuffle=True, random_state=99)\n",
    "    score = cross_val_score(clf, X_train, y_train, scoring = 'roc_auc', cv=skf, n_jobs=1)\n",
    "    return {'loss': 1-score.mean(), 'status': STATUS_OK}     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [06:55<00:00,  4.48s/it, best loss: 0.332567383379216] \n"
     ]
    }
   ],
   "source": [
    "num_eval = 100\n",
    "\n",
    "param_hyperopt= {\n",
    "                'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(2)),\n",
    "                'max_depth': scope.int(hp.quniform('max_depth', 5, 19, 1)), #'max_depth', 5, 19, 1\n",
    "                'n_estimators': scope.int(hp.quniform('n_estimators', 5, 55, 1)), #'n_estimators', 5, 55, 1\n",
    "                'num_leaves': scope.int(hp.quniform('num_leaves', 5, 38, 1)),\n",
    "                'boosting_type': hp.choice('boosting_type', ['gbdt','dart','goss']), #check lightgbm for types\n",
    "                'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0),\n",
    "                'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.5), #ridge\n",
    "                'drop_rate': hp.uniform('drop_rate', 0.0, 1.0),\n",
    "                'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.1),\n",
    "                'max_bin': scope.int(hp.quniform('max_bin', 255, 265, 1)),\n",
    "                'min_child_weight' : scope.int(hp.quniform('min_child_weight', 8, 20, 1))\n",
    "                #'scale_pos_weight': hp.uniform('scale_pos_weight', 1.0,12.0),\n",
    "                }\n",
    "\n",
    "trials = Trials()\n",
    "best_param = fmin(objective_function, \n",
    "                  param_hyperopt, \n",
    "                  algo=tpe.suggest, \n",
    "                  max_evals=num_eval, \n",
    "                  trials=trials,\n",
    "                  rstate= np.random.RandomState(1))\n",
    "loss = [x['result']['loss'] for x in trials.trials]\n",
    "best_param_values = [x for x in best_param.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.35"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(11.34532,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 0,\n",
       " 'colsample_bytree': 0.2566946790660347,\n",
       " 'drop_rate': 0.9195674755017068,\n",
       " 'learning_rate': 0.11449394808202824,\n",
       " 'max_bin': 261.0,\n",
       " 'max_depth': 12.0,\n",
       " 'min_child_weight': 19.0,\n",
       " 'n_estimators': 48.0,\n",
       " 'num_leaves': 28.0,\n",
       " 'reg_alpha': 0.9963376427069566,\n",
       " 'reg_lambda': 1.4070988716151476}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 0,\n",
       " 'colsample_by_tree': 0.5402954683529296,\n",
       " 'drop_rate': 0.7736268944483489,\n",
       " 'learning_rate': 0.02190745895434768,\n",
       " 'max_depth': 27.0,\n",
       " 'n_estimators': 54.0,\n",
       " 'num_leaves': 48.0,\n",
       " 'reg_lambda': 0.6542818431578171}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None,\n",
       "               colsample_bytree=0.2566946790660347,\n",
       "               drop_rate=0.9195674755017068, importance_type='split',\n",
       "               learning_rate=0.11449394808202824, max_bin=261, max_depth=12,\n",
       "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=48, n_jobs=-1, num_leaves=28, objective='binary',\n",
       "               random_state=None, reg_alpha=0.9963376427069566,\n",
       "               reg_lambda=1.4070988716151476, silent=True, subsample=1.0,\n",
       "               subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if best_param_values[0] == 0:\n",
    "    boosting_type = 'gbdt'\n",
    "\n",
    "elif best_param_values[0] == 1:\n",
    "    boosting_type = 'dart'\n",
    "\n",
    "else:\n",
    "    #best_param_values[0] == 2\n",
    "    boosting_type = 'goss'    \n",
    "    \n",
    "# else:\n",
    "#     boosting_type = 'goss'\n",
    "        \n",
    "    \n",
    "clf_best = lgb.LGBMClassifier(boosting_type=boosting_type, #'dart'\n",
    "                              colsample_bytree = best_param['colsample_bytree'],\n",
    "                              drop_rate = best_param['drop_rate'],\n",
    "                              learning_rate = best_param['learning_rate'],\n",
    "                              max_bin = int(best_param['max_bin']),\n",
    "                              max_depth = int(best_param['max_depth']),\n",
    "                              n_estimators = int(best_param['n_estimators']),\n",
    "                              num_leaves = int(best_param['num_leaves']),\n",
    "                              reg_alpha = best_param['reg_alpha'],\n",
    "                              reg_lambda = best_param['reg_lambda'],\n",
    "                              objective = 'binary',\n",
    "                              #scale_pos_weight = best_param['scale_pos_weight'],\n",
    "                              #bagging_fraction = \n",
    "                              )\n",
    "                                  \n",
    "clf_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Results\n",
      "Score best parameters:  -0.332567383379216\n",
      "Best parameters:  {'boosting_type': 0, 'colsample_bytree': 0.2566946790660347, 'drop_rate': 0.9195674755017068, 'learning_rate': 0.11449394808202824, 'max_bin': 261.0, 'max_depth': 12.0, 'min_child_weight': 19.0, 'n_estimators': 48.0, 'num_leaves': 28.0, 'reg_alpha': 0.9963376427069566, 'reg_lambda': 1.4070988716151476}\n",
      "Test Score:  0.9041772836074434\n",
      "Parameter combinations evaluated:  100\n",
      "AUC train is:  0.6973692029358518\n",
      "AUC test is:  0.6550757972867017\n"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "print(\"##### Results\")\n",
    "print(\"Score best parameters: \", min(loss)*-1)\n",
    "print(\"Best parameters: \", best_param)\n",
    "print(\"Test Score: \", clf_best.score(X_test, y_test))\n",
    "#print(\"Time elapsed: \", time.time() - start)\n",
    "print(\"Parameter combinations evaluated: \", num_eval)\n",
    "auc = roc_auc_score(y_test, clf_best.predict_proba(X_test)[:,1])\n",
    "auct = roc_auc_score(y_train, clf_best.predict_proba(X_train)[:,1])\n",
    "print('AUC train is: ',auct)\n",
    "print('AUC test is: ',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LGBMClassifier' object has no attribute 'plot_importance'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-aab09d85fa35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf_best\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_importance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'LGBMClassifier' object has no attribute 'plot_importance'"
     ]
    }
   ],
   "source": [
    "clf_best.plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'30u':0.6535859962867702, '30w':0.6545053440555348, '30w':0.6559087210856963}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
